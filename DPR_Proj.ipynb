{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrCn1uYtnQXYBjd5GSEB1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c6fd929ad384829bc5fefdeff04e444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f42ddb3ac1d94fc8ac1318702b438646",
              "IPY_MODEL_20b00da56a244a14a8d2479d6a21b738",
              "IPY_MODEL_b1dae74df2774982a5c71170ff2dc160"
            ],
            "layout": "IPY_MODEL_4c79b675320b4c08889565894b42ef08"
          }
        },
        "f42ddb3ac1d94fc8ac1318702b438646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df42701f783a439e884cf5e322754a13",
            "placeholder": "​",
            "style": "IPY_MODEL_87751b62453748b99948043423335a3e",
            "value": "model.safetensors: 100%"
          }
        },
        "20b00da56a244a14a8d2479d6a21b738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f5078353603468a86695b03ddd2ded7",
            "max": 714290682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea0b39d985d64df29beb4e04603c2145",
            "value": 714290682
          }
        },
        "b1dae74df2774982a5c71170ff2dc160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b7d7ba6c0a46b3837b642130230587",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa5b99588a549a4be6f2fa1b3188c45",
            "value": " 714M/714M [00:10&lt;00:00, 74.2MB/s]"
          }
        },
        "4c79b675320b4c08889565894b42ef08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df42701f783a439e884cf5e322754a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87751b62453748b99948043423335a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5078353603468a86695b03ddd2ded7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0b39d985d64df29beb4e04603c2145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80b7d7ba6c0a46b3837b642130230587": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa5b99588a549a4be6f2fa1b3188c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooking87/Data-Pattern/blob/master/%08DPR_Proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert를 이용한 가사 감정 분석\n",
        "\n",
        "## [2024 DPR Proj] IT공학전공(주전공)-4학년-2116313-손수경\n",
        "\n",
        "아래 기재한 숫자의 경우, 과제에서 요구하는 주의사항 번호임을 말씀드립니다. 또한 전체 셸 실행을 하여도 오류 없이 실행될 수 있도록 하였습니다.\n"
      ],
      "metadata": {
        "id": "c7wUpB7IT1K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 프로젝트의 motivation\n",
        "\n",
        "음악은 현대 사회에서 우리의 삶에서 더 이상 떼어 놓을 수 없는 중요한 요소가 되었습니다. 특히 유튜브 프리미엄이 등장하며 \"집중 잘되는 플레이 리스트\", \"나는 아픈 건 딱 질색이니까 1시간\" 등 음악 플레이리스트에 대한 영상이 증가하고 있습니다. <br>\n",
        "\n",
        "그 중 가사는 많은 사람들에게 위로가 될 수 있고, 기분 전환의 계기를 제공하기도 합니다. 이에 따라, 가사의 감성을 자동으로 분석하고 긍정인지 부정인지를 판단하는 것을 목표로 합니다. 이를 위해 자연어 처리 기술 중 하나인 BERT를 활용하여 감정 분석 모델을 구축할 예정입니다. <br>\n",
        "\n",
        "이 프로젝트를 통한 기대점\n",
        "1. 음악 산업에 대한 인사이트 제공: 음악에 대한 장르/감정/분위기를 제공하여 팬이 아닌 사람들도 원한느 노래를 쉽게 접하고 들어볼 수 있습니다.\n",
        "2. 자연어 처리 기술의 적용: 최근 Chat GPT로 인해 자연어 처리 기술의 정점을 찍고 있습니다. GPT와 같은 자연어 처리 기술 중 BERT를 사용함으로써 BERT에 대한 네트워크 분석, 활용을 통해 모델에 대해 공부할 수 있을 것을 예상합니다."
      ],
      "metadata": {
        "id": "VmR-f8M0m3pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 프로그램의 데이터 패턴 종류\n",
        "\n",
        "텍스트 데이터인 가사와 레이블은 긍정(1)/부정(0) 을 기준으로 학습시킬 예정입니다."
      ],
      "metadata": {
        "id": "-O_aKYXum5Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 데이터 수집 과정 및 augmentation 사용 기술\n",
        "\n",
        "genius API의 search_song 메서드를 통해서 가사를 불러왔습니다. 사용하는 최종 데이터셋(lyrics_polarity_dataset.csv)은 artist, title, year, lyrics, com_sentiment, polarity, textblob_pol 칼럼으로 구성되어 있습니다.\n",
        "\n",
        "\n",
        "- `lyrics`: 원본 가사(토큰화 전 가사)\n",
        "\n",
        "- `polarity`: vader을 통해서 com_sentiment >= 0.05 라면 pos, 아니라면 neg 로 분리하였습니다.\n",
        "\n",
        "- `textblob_pol`: textblob에서 제공하는 polarity 점수를 사용하였습니다."
      ],
      "metadata": {
        "id": "EEKwBb24V2LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. genius API 사용해서 가사 불러오기\n",
        "\n",
        "`Hot Stuff.csv` 파일 내의 `song`(노래 제목) , `performer`(가수) col과 geneius API를 통해서 해당 노래의 가사를 불러옵니다. <br/>\n",
        "\n",
        "해당 데이터는 32만개의 노래 데이터가 있으나, 빌보드 차트에 오른 노래 데이터를 기반으로 하고 있기 때문에 중복되는 경우가 많습니다. 그럼에도 원본 데이터의 양이 많기 때문에 훈련하기에 충분하다는 생각이 들었습니다. <br/>\n",
        "\n",
        "[번외] `Hot Stuff.csv` 데이터셋 csv 확인하기 <br/>\n",
        "\n",
        "`Hot Stuff.csv` 파일 출처는 [파일 출처 링크](https://data.world/kcmillersean/billboard-hot-100-1958-2017) 에 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "mWN1OJ4wUncf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXWHlqz0arbt",
        "outputId": "fc2451b1-3881-4be6-f3d2-af80ad2e118e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (9,582 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 깃허브에 올려둔 CSV 파일 다운로드\n",
        "!wget -O hot_stuff.csv \"https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/Hot_Stuff.csv\"\n",
        "\n",
        "# CSV 파일 로드\n",
        "df = pd.read_csv('hot_stuff.csv', encoding='cp949')\n",
        "\n",
        "# 데이터 확인 및 기초 통계\n",
        "print(df.shape)\n",
        "print(df.keys())\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe(include='all'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kKhL73HZYe_",
        "outputId": "12c27e86-bbbf-47a5-a8f9-2c66166347d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-30 01:46:04--  https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/Hot_Stuff.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45731337 (44M) [text/plain]\n",
            "Saving to: ‘hot_stuff.csv’\n",
            "\n",
            "hot_stuff.csv       100%[===================>]  43.61M   266MB/s    in 0.2s    \n",
            "\n",
            "2024-05-30 01:46:05 (266 MB/s) - ‘hot_stuff.csv’ saved [45731337/45731337]\n",
            "\n",
            "(327895, 10)\n",
            "                                                癤퓎rl     WeekID  \\\n",
            "0  http://www.billboard.com/charts/hot-100/1965-0...  7/17/1965   \n",
            "1  http://www.billboard.com/charts/hot-100/1965-0...  7/24/1965   \n",
            "2  http://www.billboard.com/charts/hot-100/1965-0...  7/31/1965   \n",
            "3  http://www.billboard.com/charts/hot-100/1965-0...   8/7/1965   \n",
            "4  http://www.billboard.com/charts/hot-100/1965-0...  8/14/1965   \n",
            "\n",
            "   Week Position                    Song   Performer  \\\n",
            "0             34  Don't Just Stand There  Patty Duke   \n",
            "1             22  Don't Just Stand There  Patty Duke   \n",
            "2             14  Don't Just Stand There  Patty Duke   \n",
            "3             10  Don't Just Stand There  Patty Duke   \n",
            "4              8  Don't Just Stand There  Patty Duke   \n",
            "\n",
            "                             SongID  Instance  Previous Week Position  \\\n",
            "0  Don't Just Stand TherePatty Duke         1                    45.0   \n",
            "1  Don't Just Stand TherePatty Duke         1                    34.0   \n",
            "2  Don't Just Stand TherePatty Duke         1                    22.0   \n",
            "3  Don't Just Stand TherePatty Duke         1                    14.0   \n",
            "4  Don't Just Stand TherePatty Duke         1                    10.0   \n",
            "\n",
            "   Peak Position  Weeks on Chart  \n",
            "0             34               4  \n",
            "1             22               5  \n",
            "2             14               6  \n",
            "3             10               7  \n",
            "4              8               8  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 327895 entries, 0 to 327894\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype  \n",
            "---  ------                  --------------   -----  \n",
            " 0   癤퓎rl                    327895 non-null  object \n",
            " 1   WeekID                  327895 non-null  object \n",
            " 2   Week Position           327895 non-null  int64  \n",
            " 3   Song                    327895 non-null  object \n",
            " 4   Performer               327895 non-null  object \n",
            " 5   SongID                  327895 non-null  object \n",
            " 6   Instance                327895 non-null  int64  \n",
            " 7   Previous Week Position  295941 non-null  float64\n",
            " 8   Peak Position           327895 non-null  int64  \n",
            " 9   Weeks on Chart          327895 non-null  int64  \n",
            "dtypes: float64(1), int64(4), object(5)\n",
            "memory usage: 25.0+ MB\n",
            "None\n",
            "                                                     癤퓎rl     WeekID  \\\n",
            "count                                              327895     327895   \n",
            "unique                                               3279       3279   \n",
            "top     http://www.billboard.com/charts/hot-100/1965-0...  7/17/1965   \n",
            "freq                                                  100        100   \n",
            "mean                                                  NaN        NaN   \n",
            "std                                                   NaN        NaN   \n",
            "min                                                   NaN        NaN   \n",
            "25%                                                   NaN        NaN   \n",
            "50%                                                   NaN        NaN   \n",
            "75%                                                   NaN        NaN   \n",
            "max                                                   NaN        NaN   \n",
            "\n",
            "        Week Position    Song     Performer                      SongID  \\\n",
            "count   327895.000000  327895        327895                      327895   \n",
            "unique            NaN   24360         10061                       29389   \n",
            "top               NaN    Stay  Taylor Swift  RadioactiveImagine Dragons   \n",
            "freq              NaN     208          1022                          87   \n",
            "mean        50.499309     NaN           NaN                         NaN   \n",
            "std         28.865707     NaN           NaN                         NaN   \n",
            "min          1.000000     NaN           NaN                         NaN   \n",
            "25%         25.500000     NaN           NaN                         NaN   \n",
            "50%         50.000000     NaN           NaN                         NaN   \n",
            "75%         75.000000     NaN           NaN                         NaN   \n",
            "max        100.000000     NaN           NaN                         NaN   \n",
            "\n",
            "             Instance  Previous Week Position  Peak Position  Weeks on Chart  \n",
            "count   327895.000000           295941.000000  327895.000000   327895.000000  \n",
            "unique            NaN                     NaN            NaN             NaN  \n",
            "top               NaN                     NaN            NaN             NaN  \n",
            "freq              NaN                     NaN            NaN             NaN  \n",
            "mean         1.072538               47.604066      41.358307        9.153793  \n",
            "std          0.334188               28.056915      29.542497        7.590281  \n",
            "min          1.000000                1.000000       1.000000        1.000000  \n",
            "25%          1.000000               23.000000      14.000000        4.000000  \n",
            "50%          1.000000               47.000000      39.000000        7.000000  \n",
            "75%          1.000000               72.000000      66.000000       13.000000  \n",
            "max         10.000000              100.000000     100.000000       87.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### genius API를 이용해서 가사 불러오기\n",
        "\n",
        "해당 가사를 불러오는데 시간이 너무 오래걸려 계속 코랩 연결이 끊어지는 문제점이 있었습니댜. 따라서 **전이 학습된 BERT** 를 사용하고자 하였으므로 중복없는 약 2만개의 데이터를 수집하기를 목표로 하였습니다. <br/>\n",
        "\n",
        "또한 2만개의 데이터 역시 한 번에 불러오기에는 코랩 연결 시간이 짧아 6개로 나누어 약 3334개씩 가사 크롤링을 한 후 각각의 csv 파일로 저장 후 csv 파일을 DataFrame으로 읽어와 6개의 csv 파일을 합쳐주었습니다. 그렇기에 아래 크롤링 코드는 \"이상적으로\" Hot_Stuff.csv 파일이 크롤링 되었을 때를 가정하여 코드를 작성하였습니다. <br/>\n",
        "\n",
        "데이터는 총 1주에 걸쳐 수집하였습니다. <br/>\n",
        "\n",
        "아래 코드를 통해 얻은 데이터 <br/>\n",
        "- `lyrics`: `song`(노래 제목) , `performer`(가수) col과 geneius API를 통해서 해당 노래의 가사를 불러옵니다.\n",
        "\n",
        "```pyrhon\n",
        "######### 크롤링 코드 -> 시간이 오래걸려 코드만 작성하여 두었습니다. #######################\n",
        "\n",
        "import lyricsgenius\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def get_lyrics(title, artist):\n",
        "  try:\n",
        "      return genius.search_song(title, artist).lyrics\n",
        "  except:\n",
        "      return 'not found'\n",
        "\n",
        "# Function to return sentiment score of each song\n",
        "def get_lyric_sentiment(lyrics):\n",
        "  sentiment = sid_obj.polarity_scores(lyrics)\n",
        "  return sentiment\n",
        "\n",
        "\n",
        "''' 원본 파일에서 가수, 제목을 통해서 가사 + 감정 점수 불러오기'''\n",
        "\n",
        "# genius API 사용을 위한 토큰\n",
        "genius = lyricsgenius.Genius(\n",
        "    \"lgHGVg-7RZ_QLMNZ-jeVn9lrCUfhyeR-FWe2QulGyeTU2DFYtfRVxwkGhLdCZIAa\")\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 깃허브에 올려둔 CSV 파일 다운로드\n",
        "!wget -O hot_stuff.csv \"https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/Hot_Stuff.csv\"\n",
        "\n",
        "# CSV 파일 로드\n",
        "df = pd.read_csv('hot_stuff.csv', encoding='cp949')\n",
        "\n",
        "# 사용할 col 제외하고 drop 시키기\n",
        "df = df.drop(\n",
        "    ['癤퓎rl', 'WeekID',\n",
        "    'Week Position', 'SongID', 'Instance', 'Previous Week Position', 'Peak Position', 'Weeks on Chart'], axis=1)\n",
        "\n",
        "df.drop_duplicates(subset='Song', inplace=True)\n",
        "df.reset_index(drop=True)\n",
        "\n",
        "# 노래 가사 불러오기\n",
        "lyrics = df.apply(\n",
        "    lambda row: get_lyrics(row['Song'], row['Performer']), axis=1)\n",
        "\n",
        "df['lyrics'] = lyrics\n",
        "# not found 제거\n",
        "df = df.drop(\n",
        "    df[df['lyrics'] == 'not found'].index)\n",
        "\n",
        "df.to_csv(\"lyrics_sentiment_dataset.csv\", index=False)\n",
        "```\n",
        "csv 파일로 저장 후 csv 파일을 DataFrame으로 읽어와 6개의 csv 파일을 합쳐주었다는 증거\n",
        "![split_join 증거 이미지](https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/split_join_data.png)"
      ],
      "metadata": {
        "id": "cAICRhZPb4Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQI5LvixTqjl",
        "outputId": "e1b629bf-a9d7-4c19-803c-a782a83db989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lyricsgenius\n",
            "  Downloading lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from lyricsgenius) (4.12.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from lyricsgenius) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.6.0->lyricsgenius) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->lyricsgenius) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->lyricsgenius) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->lyricsgenius) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->lyricsgenius) (2024.2.2)\n",
            "Installing collected packages: lyricsgenius\n",
            "Successfully installed lyricsgenius-3.0.1\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade lyricsgenius\n",
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. SentimentIntensityAnalyzer 클래스를 사용하여 compound 감정 점수 가져오기\n",
        "\n",
        "- vaderSentiment: `vaderSentiment`는 감성 분석을 위해 개발된 VADER (Valence Aware Dictionary and sEntiment Reasoner) 도구를 제공합니다. VADER는 특히 소셜 미디어 텍스트와 같은 짧은 텍스트에서 감정을 분석하는 데 유용합니다.\n",
        "    - compound 점수: VADER의 \"compound\" 점수는 텍스트의 전반적인 감성을 나타내는 하나의 정규화된 값입니다. 이 점수는 -1(매우 부정적)에서 +1(매우 긍정적) 사이의 값을 가집니다. 텍스트의 각 단어의 감성 점수의 합을 계산 후 정규화하여 -1에서 +1 사이의 값으로 변환해주는 값입니다. 일반적을 0.05 이상의 경우는 긍정을, 0.05이하의 경우는 부정을 의미합니다. <br />\n",
        "\n",
        "아래 코드를 통해 얻을 수 있는 데이터 <br />\n",
        "    - `com_sentiment`: caderSentiment를 통해서 가져온 compound 점수 <br />\n",
        "    - `polarity`: `com_sentiment`가 0.05이상의 경우는 긍정(1)로, 0.05이하의 경우는 부정(0)으로 표현하였습니다. <br />\n",
        "    - `textblob_pol`: 문장의 감정 점수를 계산하여 각 문장의 긍정/부정성을 측정하여 칼럼을 추가하였습니다. <br/>\n",
        "\n",
        "데이터를 학습시키는 과정에서 성능이 더 좋은 칼럼을 선택하기 위해서 비슷한 기능을 하는 칼럼 2개를 추가하였습니다."
      ],
      "metadata": {
        "id": "GVfQwkOwhUz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "# Function to return sentiment score of each song\n",
        "def get_lyric_sentiment(lyrics):\n",
        "  sentiment = sid_obj.polarity_scores(lyrics)\n",
        "  return sentiment\n",
        "\n",
        "# textblob 패키지로 감정 분석하기\n",
        "def sent_textblob_polarity(lyrics):\n",
        "    polarity_score = TextBlob(lyrics).sentiment.subjectivity\n",
        "    if float(polarity_score) > 0:\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "# vader\n",
        "def sent_vader_compound(com):\n",
        "  if float(com) >= 0.05:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "# 깃허브에 올려둔 CSV 파일 다운로드\n",
        "!wget -O lyrics_sentiment_dataset.csv \"https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/lyrics_sentiment_dataset.csv\"\n",
        "\n",
        "# CSV 파일 로드\n",
        "lyrics_sentiment_dataset = pd.read_csv('lyrics_sentiment_dataset.csv')\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Use get_lyric_sentiment to get sentiment score for all the song lyrics\n",
        "# SentimentIntensityAnalyzer 클래스를 이용해서 가사의 감정 수치를 생성한다.\n",
        "sentiment = lyrics_sentiment_dataset.apply(\n",
        "    lambda row: get_lyric_sentiment(row['lyrics']), axis=1)\n",
        "# df에 com_sentiment col 추가하기\n",
        "for i in lyrics_sentiment_dataset.index.tolist():\n",
        "    lyrics_sentiment_dataset.loc[i, 'com_sentiment'] = sentiment[i]['compound']\n",
        "\n",
        "# com_sentiment를 통핸 긍정/부정 분리 및 textblob을 통한 또다른 긍정/부정 col 생성\n",
        "for i in lyrics_sentiment_dataset.index.tolist():\n",
        "  lyrics = lyrics_sentiment_dataset.loc[i, \"lyrics\"]\n",
        "  lyrics_sentiment_dataset.loc[i, \"polarity\"] = sent_vader_compound(lyrics_sentiment_dataset.loc[i, \"com_sentiment\"])\n",
        "  lyrics_sentiment_dataset.loc[i, \"textblob_pol\"] = sent_textblob_polarity(lyrics)\n",
        "\n",
        "# 저는 실행마다 첫 번째 셸부터 돌리기에는 시간상 비효율이라는 생각이 들어 `lyrics_polarity_dataset.csv` 파일을 만들었습니다.\n",
        "lyrics_sentiment_dataset.to_csv(\"lyrics_polarity_dataset.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crql_10Lk-qt",
        "outputId": "9eef9b15-c6c6-4a7c-cc4b-7d98a788f368"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-30 01:46:23--  https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/lyrics_sentiment_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28817888 (27M) [text/plain]\n",
            "Saving to: ‘lyrics_sentiment_dataset.csv’\n",
            "\n",
            "lyrics_sentiment_da 100%[===================>]  27.48M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-30 01:46:24 (223 MB/s) - ‘lyrics_sentiment_dataset.csv’ saved [28817888/28817888]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentimentIntensityAnalyzer 클래스 샘플 사용 결과 보기 <br/>\n",
        "\n",
        "아래 가사는 긍정 가사를 검색 후 감정 분석을 진행한 결과입니다."
      ],
      "metadata": {
        "id": "qvld5kqjG7gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_lyric_sentiment(lyrics):\n",
        "  sentiment = sid_obj.polarity_scores(lyrics)\n",
        "  return sentiment\n",
        "\n",
        "lyrics = '''\n",
        "Hope it's all the I have\n",
        "The calm in the storm\n",
        "Is right where I am\n",
        "Inside of hope\n",
        "It's all that I know\n",
        "It's what I'm moved toward\n",
        "It's what I live for\n",
        "\n",
        "Wave goodbye to a time\n",
        "That you once believed was everything\n",
        "\n",
        "I'm ready to live\n",
        "I'm ready to dream\n",
        "I'm ready for fear, and love and everything between\n",
        "\n",
        "Don't tell me I'm right\n",
        "Don't tell me I'm wrong\n",
        "Just tell me I'm strong enough for one more song\n",
        "And this could be the one to say it right\n",
        "Bringing warmth into the night\n",
        "I'm going to broke, but I've got hope\n",
        "\n",
        "'''\n",
        "\n",
        "sent = get_lyric_sentiment(lyrics)\n",
        "print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b45q_MOXc5Qj",
        "outputId": "b1e31782-5b57-492c-f0ff-30388be813d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.05, 'neu': 0.759, 'pos': 0.191, 'compound': 0.9161}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 데이터의 크기 및 개수, 용량, 데이터 크기의 타당성 (딥러닝 가능성)"
      ],
      "metadata": {
        "id": "4hRbYLRxm-6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 전이학습이나, 타인의 데이터를 추가한 경우, 그 데이터에 대한 설명\n",
        "\n",
        "- 전이 학습 모델 중 Bert를 사용할 예정입니다. 자세한 사항에 대해서는 아래 6번에서 설명드리겠습니다.\n",
        "- 타인의 데이터를 추가한 것은 없습니다."
      ],
      "metadata": {
        "id": "ddvmvhQ_mtek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 본인의 딥러닝 프로그래밍에 대한 자세한 설명 (딥네트웍 구조 등)\n",
        "\n",
        "### BERT Explained: State of the art language model for NLP\n",
        "\n",
        "<https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270> <br>\n",
        "\n",
        "#### How BERT works\n",
        "\n",
        "BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. <br>\n",
        "<br>\n",
        "\n",
        "As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). <br>\n",
        "<br>\n",
        "\n",
        "When training language models, there is a challenge of defining a prediction goal. 빈칸에 들어가는 단어를 예측하는데 어려움이 있다. 왜? directional approach 자체의 한계점이다. 이를 극복하기 위해서 BERT는 2가지 전략을 사용한다.\n",
        "\n",
        "1. Masked LM (MLM)\n",
        "\n",
        "   Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n",
        "\n",
        "   ![download1](https://user-images.githubusercontent.com/96654391/188531248-8bf6007f-0289-434d-a94f-79556ef41e40.png)\n",
        "\n",
        "   1. Adding a classification layer on top of the encoder output.\n",
        "   2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
        "   3. Calculating the probability of each word in the vocabulary with softmax. <br>\n",
        "\n",
        "2. Next Sentence Prediction (NSP)\n",
        "\n",
        "   During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence. <br>\n",
        "   이런 문제가 있어서 BERT에서는 문장 시작 부분에 [CLS] 끝나는 부분에 [SEP]을 붙혀서 토큰화, 임베딩을 한다. <br>\n",
        "\n",
        "   ![download2](https://user-images.githubusercontent.com/96654391/188532302-3ae6c9dc-f3a2-42b8-a662-32450e7e3ce5.png) <br>\n",
        "\n",
        "   To predict if the second sentence is indeed connected to the first, the following steps are performed: <br>\n",
        "\n",
        "   1. The entire input sequence goes through the Transformer model.\n",
        "   2. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
        "   3. Calculating the probability of IsNextSequence with softmax.\n",
        "\n",
        "#### How to use BERT (Fine-tuning)\n",
        "\n",
        "In the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section 3.5) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks, detailed in Section 4 of the paper. <br>\n",
        "\n",
        "BERT’s bidirectional approach (MLM) converges slower than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.\n",
        "\n",
        "\n",
        "\n",
        "#### How BERT works\n",
        "\n",
        "The transformer is the part of the model that gives BERT its increased capacity for understanding context and ambiguity in language. The transformer does this by processing any given word in relation to all other words in a sentence, rather than processing them one at a time. the Transformer allows the BERT model to understand the full context of the word, and therefore better understand searcher intent. <br>\n",
        "\n",
        "BERT uses a method of masked language modeling to keep the word in focus from \"seeing itself\" -- that is, having a fixed meaning independent of its context.\n",
        "\n",
        "\n",
        "\n",
        "#### What is BERT Tokenizer?\n",
        "\n",
        "The tokenizers should also match the core model that we would like to use as the pre-trained, e.g. cased and uncased version. <br>\n",
        "\n",
        "<https://huggingface.co/docs/transformers/model_doc/bert#berttokenizer> : BERT 하이퍼 파라미터 정리 <br>\n",
        "\n",
        "#### from_pretrained()\n",
        "\n",
        "Transformers를 통해 저장된 모델은 기본적으로 pretrained model, tokenizer, vocab, config 파일 등을 포함하고 있으며, from_pretrained() 메소드를 통해 로드할 수 있다. <br>\n",
        "\n",
        "***결론적으로 tokenizer를 이용해서 태그를 추가해준 후 모델 훈련을 시킬 수 있다.***"
      ],
      "metadata": {
        "id": "6SR-rH4Dne3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddvw1AlYbhIq",
        "outputId": "5627bc86-ae4c-42fd-e8af-2e53263eb574"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1. Bert에 넣기 위한 input 데이터 전처리(태그 추가)\n",
        "\n",
        "저는 실행마다 첫 번째 셸부터 돌리기에는 시간상 비효율이라는 생각이 들어 `lyrics_polarity_dataset.csv` 파일을 만들었습니다. <br>"
      ],
      "metadata": {
        "id": "esqP_0IKpX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# 깃허브에 올려둔 CSV 파일 다운로드\n",
        "!wget -O lyrics_polarity_dataset.csv \"https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/lyrics_polarity_dataset.csv\"\n",
        "\n",
        "# CSV 파일 로드\n",
        "df = pd.read_csv('lyrics_polarity_dataset.csv')\n",
        "\n",
        "# train-set, test-set 분리\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['polarity'], test_size=0.25, random_state=32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['textblob_pol'], test_size=0.25, random_state=32)\n",
        "print(X_train.shape)\n",
        "\n",
        "# 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "\n",
        "def tokenize_data(texts, labels, tokenizer, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = tf.concat(input_ids, axis=0)\n",
        "    attention_masks = tf.concat(attention_masks, axis=0)\n",
        "    labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Train 데이터 토큰화\n",
        "train_input_ids, train_attention_masks, train_labels = tokenize_data(X_train, y_train, tokenizer)\n",
        "\n",
        "# Test 데이터 토큰화\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_data(X_test, y_test, tokenizer)\n",
        "\n",
        "# TensorFlow 데이터셋 생성\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_masks}, train_labels)).batch(16)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': test_input_ids, 'attention_mask': test_attention_masks}, test_labels)).batch(16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYRaAccAcCiC",
        "outputId": "0a73cfa4-2f5b-4a47-c675-aa3dbeafa079"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-30 02:02:12--  https://raw.githubusercontent.com/sooking87/Data-Pattern/master/Data-Preprocessing/lyrics_polarity_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28990576 (28M) [text/plain]\n",
            "Saving to: ‘lyrics_polarity_dataset.csv’\n",
            "\n",
            "lyrics_polarity_dat 100%[===================>]  27.65M  86.3MB/s    in 0.3s    \n",
            "\n",
            "2024-05-30 02:02:12 (86.3 MB/s) - ‘lyrics_polarity_dataset.csv’ saved [28990576/28990576]\n",
            "\n",
            "(16308,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2. Model Compile and Fit\n",
        "\n",
        "** 첫 번째 초청강의 모델 가중치 저장 방법 사용하기"
      ],
      "metadata": {
        "id": "Hk7oBgqCrhMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# 모델 초기화\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 옵티마이저, 손실 함수, 메트릭 정의\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(train_dataset, epochs=3, validation_data=test_dataset)\n",
        "\n",
        "# 모델 가중치 저장\n",
        "model.save_weights(\"textblob_final_model_weights.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "8c6fd929ad384829bc5fefdeff04e444",
            "f42ddb3ac1d94fc8ac1318702b438646",
            "20b00da56a244a14a8d2479d6a21b738",
            "b1dae74df2774982a5c71170ff2dc160",
            "4c79b675320b4c08889565894b42ef08",
            "df42701f783a439e884cf5e322754a13",
            "87751b62453748b99948043423335a3e",
            "6f5078353603468a86695b03ddd2ded7",
            "ea0b39d985d64df29beb4e04603c2145",
            "80b7d7ba6c0a46b3837b642130230587",
            "8fa5b99588a549a4be6f2fa1b3188c45"
          ]
        },
        "id": "B9GlR6z7rtGK",
        "outputId": "5785bd30-94fe-4459-8eb8-9bb26f4b5665"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c6fd929ad384829bc5fefdeff04e444",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7e5b3832dea0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7e5b3832dea0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            " 104/1020 [==>...........................] - ETA: 6:15:42 - loss: 0.1666 - accuracy: 0.9621"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-3. Model Load"
      ],
      "metadata": {
        "id": "vJWYs_9JEXqW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57nAt4U9Eadh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 결과에 대한 충분한 분석과 설명(도표나, 결과 화면을 캡춰해서 제시)"
      ],
      "metadata": {
        "id": "DQfAOiWbr1oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 깃허브에 가중치 파일은 용량이 커 업로드가 되지 않습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abz4SKdFR9OF",
        "outputId": "319056db-99b1-4185-832f-b32936348aaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXD_BzRMScgM",
        "outputId": "444c1a94-08a4-4796-e0b6-6c91aeb446ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 옵티마이저, 손실 함수, 메트릭 정의\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# 가중치 파일 경로\n",
        "weights_path = '/content/drive/MyDrive/Colab Notebooks/DPR Proj/vader_final_model_weights.h5'\n",
        "model.load_weights(weights_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL6jooC4SIoO",
        "outputId": "441b2483-bc66-405c-f3b7-49635fbf6c69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "# 새로운 텍스트 예측\n",
        "texts = [\n",
        "    '''Hope it's all the I have\n",
        "    The calm in the storm\n",
        "    Is right where I am\n",
        "    Inside of hope\n",
        "    It's all that I know\n",
        "    It's what I'm moved toward\n",
        "    It's what I live for\n",
        "\n",
        "    Wave goodbye to a time\n",
        "    That you once believed was everything\n",
        "\n",
        "    I'm ready to live\n",
        "    I'm ready to dream\n",
        "    I'm ready for fear, and love and everything between\n",
        "\n",
        "    Don't tell me I'm right\n",
        "    Don't tell me I'm wrong\n",
        "    Just tell me I'm strong enough for one more song\n",
        "    And this could be the one to say it right\n",
        "    Bringing warmth into the night\n",
        "    I'm going to broke, but I've got hope'''\n",
        "]\n",
        "# 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "encoded_texts = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "predictions = model.predict(encoded_texts)\n",
        "# 로짓을 소프트맥스 함수로 확률로 변환\n",
        "probs = tf.nn.softmax(predictions.logits, axis=-1)\n",
        "predicted_labels = tf.argmax(probs, axis=1)\n",
        "\n",
        "print(f\"Predicted label: {predicted_labels[0].numpy()} with probability: {probs[0][predicted_labels[0]].numpy() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrvZkRNNSvYK",
        "outputId": "e11938cf-22ee-48ec-a73b-15bf65150553"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 506ms/step\n",
            "Predicted label: 1 with probability: 99.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "# 새로운 텍스트 예측\n",
        "texts = [\n",
        "    '''I am bad boy but I am happy'''\n",
        "]\n",
        "# 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "encoded_texts = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "predictions = model.predict(encoded_texts)\n",
        "# 로짓을 소프트맥스 함수로 확률로 변환\n",
        "probs = tf.nn.softmax(predictions.logits, axis=-1)\n",
        "predicted_labels = tf.argmax(probs, axis=1)\n",
        "\n",
        "print(f\"Predicted label: {predicted_labels[0].numpy()} with probability: {probs[0][predicted_labels[0]].numpy() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f45bf9c-d472-4c6e-d0bc-da763a03a1ea",
        "id": "ABFmGMo7URxp"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 178ms/step\n",
            "Predicted label: 0 with probability: 64.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 프로젝트를 위해 활용한 자료나, 동영상 링크\n",
        "\n",
        "- Fine-tune BERT Model for Sentiment Analysis in Google Colab: <https://www.analyticsvidhya.com/blog/2021/12/fine-tune-bert-model-for-sentiment-analysis-in-google-colab/#:~:text=Introduction%20to%20BERT%20Model%20for,negative%2C%20or%20neutral%20about%20it.>\n",
        "\n",
        "- BERT 설명 pdf: <https://arxiv.org/pdf/1706.03762.pdf>\n",
        "\n",
        "- BERT language model: <https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection>\n",
        "\n",
        "- Transformer와 Tensorflow를 가지고 fine-tunning 해보기: <https://velog.io/@jaehyeong/Fine-tuning-Bert-using-Transformers-and-TensorFlow>\n"
      ],
      "metadata": {
        "id": "3J7rF8jFtEsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. 본 프로젝트를 통해 느낀점\n",
        "\n",
        "데이터 수집부터 모델 훈련 및 결과 도출까지의 과정은 이번 프로젝트에서 처음 경험하는 일이었습니다. 프로젝트를 진행하면서, 모델을 불러와 사용하는 시간보다 데이터를 수집하고 전처리하는 과정이 훨씬 더 오래 걸린다는 것을 깨달았습니다. <br>\n",
        "\n",
        "특히, 데이터 수집 과정은 예상했던 일정보다 훨씬 더 많은 시간이 소요되었습니다. 처음에는 가사와 감정 라벨이 포함된 데이터를 찾으려 했으나, 해당 데이터셋은 양이 적거나 가사 원본이 포함되지 않는 경우가 많았습니다. 결국, 가수와 노래 제목을 기준으로 크롤링을 진행하기로 결정했습니다. <br>\n",
        "\n",
        "그러나 크롤링 과정도 쉽지 않았습니다. 가사를 크롤링하는 데 생각보다 많은 시간이 소요되었고, 하루 종일 노트북을 켜놓지 않으면 약 2만 개의 데이터를 한 번에 수집하기 어려웠습니다. 그래서 데이터를 6등분하여 약 3,000개씩 나누어 수집하고, 백업용으로 CSV 파일에 저장했습니다. <br>\n",
        "\n",
        "어렵게 데이터를 수집한 후, 전처리하여 BERT 모델에 입력하려고 했습니다. 일반적으로 자연어 처리에서는 불용어(stopword)를 제거하고 표제어를 추출하는 과정이 필수적입니다. 이러한 전처리 과정을 거쳐 데이터를 준비하려 했으나, BERT에 대해 더 깊이 공부하면서 토크나이저(tokenizer)를 통해 데이터를 임베딩하고 입력할 수 있다는 사실을 알게 되었습니다. <br>\n",
        "\n",
        "이에 따라 전처리 단계를 최소화하고 원본 데이터를 토크나이저를 통해 BERT에 적합한 입력 형태로 변환하였습니다. 이로 인해 모델의 정확도를 크게 향상시킬 수 있었습니다. BERT 모델에 대해 학습한 덕분에, 생각보다 빠르게 모델의 성능을 개선할 수 있었고, 그 결과에 대해 매우 만족스러웠습니다. <br>\n",
        "\n",
        "이 프로젝트를 통해 데이터 수집 및 전처리의 중요성을 실감할 수 있었고, BERT 모델의 강력한 자연어 처리 능력을 직접 경험할 수 있었습니다. 앞으로 이러한 경험을 바탕으로 더욱 효율적이고 정확한 자연어 처리 프로젝트를 수행할 수 있을 것이라 기대합니다.\n"
      ],
      "metadata": {
        "id": "NiZSbgf5tvBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. 프로젝트 수행 과정에 대한 간략한 일정 고찰\n",
        "\n",
        "1. 데이터 수집(1.5주)\n",
        "    - 처음 3일간 원하는 데이터가 있는지를 찾아보았습니다.\n",
        "    - 원하는 데이터가 없어 크롤링을 시도하였습니다.\n",
        "        - 도전 과제: 크롤링 속도가 느려 데이터 수집에 많은 시간이 소요되었습니다. 이를 해결하기 위해 데이터를 나누어 병렬로 수집하고 백업 파일(csv)을 생성하였습니다.\n",
        "\n",
        "2. 데이터 전처리 (0.5주)\n",
        "\n",
        "    - 데이터 정제: 수집한 데이터에서 불필요한 문자나 특수 문자를 제거합니다.\n",
        "    - 토크나이징: BERT 모델에 맞게 데이터를 토크나이저를 통해 토크나이징합니다.\n",
        "        - 도전 과제: 불용어 제거와 표제어 추출의 불필요성을 인지 후, 토크나이저를 사용하여 보다 효율적인 전처리 방법을 적용하였습니다.\n",
        "\n",
        "3. 모델 훈련 및 모델 평가/튜닝(0.5주)\n",
        "    - 모델 평가: 초기에 데이터 수집 시 polarity, textblob_pol 칼럼을 생성하였는데, 각각의 칼럼을 레이블로 사용하여 높은 성능을 가진 가중치를 사용하였습니다.\n",
        "    - 아쉬운 점: 수업 시간에 배운 교차 검증을 사용하여 성능 비교를 해보고 싶었으나, 훈련시간이 너무 오래걸리고, 제한된 하드웨어의 이유로 실험을 해보지 못한 점이 아쉬웠습니다."
      ],
      "metadata": {
        "id": "E-H0VCRZtxrI"
      }
    }
  ]
}